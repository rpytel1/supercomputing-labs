Lab 1 {#lab-1 .unnumbered}
=====

In this lab, we will design and develop an application to process GDELT data.
For a given amount of segments, the application should output the 10 most
frequently mentioned topics contained in the `AllNames` column. The application
will later be used in lab 2 to scale the analysis to the entire dataset.


## Before you start

We recommend you read all the relevant sections on Scala and Spark in the
guide. Make sure you have Docker up-and-running and that you have built the
required images for Spark and SBT, as per the instructions. You can verify your
set-up by going through the steps of the Spark tutorial.

Download the template project from [lab's GitHub repository] (contained in
the `lab1/` folder), either by forking or cloning the repository, or
downloading a zip file. You should execute all Docker commands from this
project folder. Rename the Scala file and the class it contains to something
meaningful. Update the project name in `build.sbt`. The project folder also
contains a `data/` directory, which will contain the data you download for
testing, as explained in the following. The `data/` folder is automatically
mounted in the working directory of relevant containers.


## Assignment

We will use the GDELT version 2 GKG files. As mentioned, these files are
formatted in tab-separated values. The schema is listed below. The columns that
are most relevant are `DATE` and `AllNames`.

To use the schema for Spark's `DataFrameReader`, first make sure to import the
required Spark SQL types:

```{.scala}
import org.apache.spark.sql.types._
```

The schema can then be specified as a Spark `StructType`, as listed below.

```{.scala}
val schema = StructType(
      Array(
        StructField("GKGRECORDID", StringType),
        StructField("DATE", DateType),
        StructField("SourceCollectionIdentifier", IntegerType),
        StructField("SourceCommonName", StringType),
        StructField("DocumentIdentifier", StringType),
        StructField("Counts", StringType),
        StructField("V2Counts", StringType),
        StructField("Themes", StringType),
        StructField("V2Themes", StringType),
        StructField("Locations",StringType),
        StructField("V2Locations",StringType),
        StructField("Persons",StringType),
        StructField("V2Persons",StringType),
        StructField("Organizations",StringType),
        StructField("V2Organizations",StringType),
        StructField("V2Tone", StringType),
        StructField("Dates",StringType),
        StructField("GCAM", StringType),
        StructField("SharingImage", StringType),
        StructField("RelatedImages",StringType),
        StructField("SocialImageEmbeds",StringType),
        StructField("SocialVideoEmbeds",StringType),
        StructField("Quotations", StringType),
        StructField("AllNames", StringType),
        StructField("Amounts",StringType),
        StructField("TranslationInfo",StringType),
        StructField("Extras", StringType)
      )
```

In the `data/` folder you will also find a script called `get_data` and its
Powershell equivalent `get_data.ps1`, for use on Windows. This script will
download a number of GKG segments to your computer, and generate a file
`local_index.txt` containing the paths to the downloaded files.

```{.sh}
$ ./get_data 4
...
wget downloading
...

$ cat local_index.txt
/path/to/this/repo/lab1/data/segment/20150218230000.gkg.csv
/path/to/this/repo/lab1/data/segment/20150218231500.gkg.csv
/path/to/this/repo/lab1/data/segment/20150218233000.gkg.csv
/path/to/this/repo/lab1/data/segment/20150218234500.gkg.csv
```

The script will put all downloaded files in the `segment` folder. `wget`
adds timestamps to downloads, so files will not be needlessly downloaded again.

You can use these local files for the first lab assignment to test your
application, and build some understanding of the scaling behaviour on a single
machine.

Your program should output a structure that maps dates to a list of 10 tuples
containing the most mentioned topics and the amount of times they were
mentioned. An example output of this system based on 10 segments would be:

```
DateResult(2015-02-19,List((United States,1497), (Islamic State,1233), (New
York,1058), (United Kingdom,735), (White House,723), (Los Angeles,620), (New
Zealand,590), (Associated Press,498), (San Francisco,479), (Practice Wrestling
Room,420)))
DateResult(2015-02-18,List((Islamic State,1787), (United States,1210), (New
York,727), (White House,489), (Los Angeles,424), (Associated Press,385), (New
Zealand,353), (United Kingdom,325), (Jeb Bush,298), (Practice Wrestling
Room,280)))
```

Or in JSON:

```{.json}
{"data":"2015-02-19","result":[{"topic":"United
States","count":1497},{"topic":"Islamic State","count":1233},{"topic":"New
York","count":1058},{"topic":"United Kingdom","count":735},{"topic":"White
House","count":723},{"topic":"Los Angeles","count":620},{"topic":"New
Zealand","count":590},{"topic":"Associated Press","count":498},{"topic":"San
Francisco","count":479},{"topic":"Practice Wrestling Room","count":420}]}
{"data":"2015-02-18","result":[{"topic":"Islamic
State","count":1787},{"topic":"United States","count":1210},{"topic":"New
York","count":727},{"topic":"White House","count":489},{"topic":"Los
Angeles","count":424},{"topic":"Associated Press","count":385},{"topic":"New
Zealand","count":353},{"topic":"United Kingdom","count":325},{"topic":"Jeb
Bush","count":298},{"topic":"Practice Wrestling Room","count":280}]}
```

The exact counts can vary depending on how you count, for instance if a name
is mentioned multiple times per article, do you count it once or multiple
times? Something in between? Do you filter out some names that are false
positives ("ParentCategory" seems to be a particular common one)? You are free
to implement it whatever you think is best, and are encouraged to experiment
with this. Document your choices in your report.

## Deliverables

The deliverables for the first lab are:

1.  A Dataframe/Dataset-based implementation of the GDELT analysis,
2.  An RDD-based implementation of the GDELT analysis,
3.  A report containing:
    1.  Outline of your implementation and approach (½–1 page);
    2.  Answers to the questions listed below. Be as concise as you can!

For the implementations, **only** hand in your Scala files. Your code should
run **without** changes in the rest of the project. Your submission should
contain 2 Scala files and 1 PDF file containing your report.

The deadline of this lab will be announced on Brightspace. Your report and code
will be discussed in a brief oral examination during the lab, the schedule of
which will become available later.


## Questions

General questions:

1.  In typical use, what kind of operation would be more expensive, a narrow
    dependency or a wide dependency? Why? (max. 50 words)
2.  What is the shuffle operation and why is it such an important topic in
    Spark optimization? (max. 100 words)
3.  In what way can Dataframes and Datasets improve performance both in
    compute, but also in the distributing of data compared to RDDs? Under what
    conditions will Dataframes perform better than RDDs? (max. 100 words)
4.  Consider the following scenario. You are running a Spark program on a big
    data cluster with 10 worker nodes and a single master node. One of the
    worker nodes fails. In what way does Spark's programming model help you
    recover the lost work? (Think about the execution plan or DAG) (max. 50
    words)
5.  We might distinguish the following five conceptual parallel programming
    models:

    1.  farmer/worker
    2.  divide and conquer
    3.  data parallelism
    4.  function parallelism
    5.  bulk-synchronous

    Pick one of these models and explain why it does or does not fit Spark's
    programming model. (max. 100 words)

Implementation analysis questions:

1.  Measure the execution times for 10, 20, 50, 100 and 150 segments. Do you
    observe a difference in execution time between your Dataframe and RDD
    implementations? Is this what you expect and why? (max. 50 words)
2.  How will your application scale when increasing the amount of analyzed
    segments? What do you expect the progression in execution time will be for, 100,
    1000, 10000 segments? (max. 50 words)
3.  If you extrapolate the scaling behavior on your machine, using your results
    from question 1, to the entire dataset, how much time will it take to
    process the entire dataset? Is this extrapolation reasonable for a single
    machine? (max. 50 words)
4.  Now suppose you had a cluster of identical machines with that you performed the
    analysis on. How many machines do you think you would need to process the
    entire dataset in under an hour? Do you think this is a valid
    extrapolation? (max. 50 words)
5.  Suppose you would run this analysis for a company. What do you think would
    be an appropriate way to measure the performance? Would it be the time it
    takes to execute? The amount of money it takes to perform the analysis on
    the cluster? A combination of these two, or something else? Pick something
    you think would be an interesting metric, as this is the metric you will be
    optimizing in the 2nd lab! (max. 100 words)

  [GitHub Student Developer Pack]: https://education.github.com/pack
  [Spark overview page]: https://spark.apache.org/docs/latest/
  [lab's GitHub repository]: https://github.com/Tclv/SBD-tudelft
  [Standard Big Data pipeline.]: ./images/big_data_stages
  [AWS CLI]: http://docs.aws.amazon.com/cli/latest/userguide/installing.html
  [AWS-CLI docs]: https://aws.amazon.com/cli/
  [Reddit AMA]: https://www.reddit.com/r/IAmA/comments/31bkue/im_matei_zaharia_creator_of_spark_and_cto_at/
  [AWS site]: https://aws.amazon.com/premiumsupport/knowledge-center/emr-pyspark-python-3x/
  [AWS website]: http://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-web-interfaces.html
  [AWS documentation]: http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-spot-limits.html
  [Scala language site]: https://docs.scala-lang.org/tour/tour-of-scala.html
  [lazy evaluation]: https://en.wikipedia.org/wiki/Lazy_evaluation
  [Spark SQL, DataFrames and Datasets Guide]: https://spark.apache.org/docs/latest/sql-programming-guide.html#overview
  [Spark SQL types]: https://spark.apache.org/docs/2.3.1/api/java/org/apache/spark/sql/types/package-frame.html
  [Java date format]: https://docs.oracle.com/javase/8/docs/api/java/text/SimpleDateFormat.html
  [case class]: https://docs.scala-lang.org/tour/case-classes.html
  [RDDs]: https://spark.apache.org/docs/latest/rdd-programming-guide.html
  [Dataframes/Datasets]: https://spark.apache.org/docs/latest/sql-programming-guide.html
  [Spark documentation]: https://spark.apache.org/docs/2.3.1/api/scala/index.html#package
  [GKG codebook]: http://data.gdeltproject.org/documentation/GDELT-Global_Knowledge_Graph_Codebook-V2.1.pdf
  [log levels to warn]: https://stackoverflow.com/questions/27781187/how-to-stop-info-messages-displaying-on-spark-console
  [SBT build tool]: https://www.scala-sbt.org
