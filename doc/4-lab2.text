# Lab 2 {.unnumbered}

In the first lab you built an application for a small dataset to analyze the
most common topics in the news according to the GDelt dataset. In this lab we
will scale this application using Amazon Web Services to process the entire
dataset (several terabytes). You are free to pick either your RDD, or
Dataframe/Dataset implementation.

**We assume everybody has access to AWS credits via both the GitHub developer
pack and the AWS classroom in their lab group! If this is not the case, please
ask for help, or send us an email.**

**You pay for cluster per commissioned minute. After you are done working with
a cluster, please terminate the cluster, to avoid unnecessary costs.**

Like last lab, we will first give you a short description of the different
technologies you will be using before we give the actual assignment.

## Amazon Web Services

AWS consists of a variety of different services, the ones relevant for this lab
are listed below:

[EC2]
:   Elastic Compute Cloud allows you to provision a variety of different
    machines that can be used to run a computation. An overview of the
    different machines and their use cases can be found on the EC2 website.

[EMR]
:   Elastic MapReduce is a layer on top of EC2, that allows you to quickly
    deploy MapReduce-like applications, for instance Apache Spark.

[S3]
:   Simple Storage Server is an object based storage system that is easy to
    interact with from different AWS services.

Note that the GDelt GKG is hosted on AWS S3 in the [US east region], so any
EC2/EMR instances interacting with this data set should also be provisioned
there. At the time of writing, this means that you should select either the
Virginia or Ohio region for your instances.

AWS EC2 offers spot instances, a marketplace for unused machines that you can
bid on. These spot instances are often a order of magnitude cheaper than
on-demand instances. The current price list can be found in the [EC2 website].
We recommend using spot instances for the entirety of this lab.

We will be using the AWS infrastructure to run the application. Log in to the AWS
console, and open the S3 interface. Create a bucket where we can store the
application JAR, and all the other files needed by your application.

There are (at least) two ways to transfer files to S3:

1.  The web interface, and
2.  The command line interface.

The web interface is straightforward to use. To use the command line interface,
first install the [AWS CLI]. Some example operations are listed below.

To copy a file

``` {.bash}
aws s3 cp path/to/file s3://destination-bucket/path/to/file
```

To copy a directory recursively

``` {.bash}
aws s3 cp --recursive s3://origin-bucket/path/to/file
```

To move a file

``` {.bash}
aws s3 mv path/to/file s3://destination-bucket/path/to/file
```

The aws-cli contains much more functionality, which can be found on the
[AWS-CLI docs].

Once you have uploaded all the necessary files (again your application JAR, and
all the files required by the application).

We are now ready to provision a cluster. Go to the EMR service, and select
*Create Cluster*. Next select *Go to advanced options*, select the latest
release, and check the frameworks you want to use. In this case this means
Spark, Hadoop and Ganglia. Spark and Hadoop you already know, we will introduce
Ganglia later in this chapter.

EMR works with steps, which can be thought of as a job, or the execution of a
single application. You can choose to add steps in the creation of the cluster,
but this can also be done at a later time. Press *next*.

In the *Hardware Configuration* screen, we can configure the arrangement and
selection of the machines. We suggest starting out with `m4.large` machines on
spot pricing. You should be fine running a small example workload with a single
master node and two core nodes.[^1][^2] Be sure to select *spot pricing* and
place an appropriate bid. Remember that you can always check the current prices
in the information popup or on the [Amazon website][EC2 website]. After
selecting the machines, press *next*.

[^1]: You always need a master node, which is tasked with distributing
    resources and managing tasks for the core nodes. We recommend using
    the cheap `m4.large` instance. If you start to notice unexplained
    bottlenecks for tasks with many machines and a lot of data, you might want
    to try a larger master node. Ganglia should provide you with some insights
    regarding this matter.

[^2]: By default, there are some limitations on the number of spot instances
    your account is allowed to provision. If you don't have access to enough
    spot instances, the procedure to request additional can be found in the
    [AWS documentation].

In the *General Options* you can select a cluster name. You can tune where the
system logs and a number of other features (more information in the popups).
After finishing this step, press *next*.

You should now arrive in the *Security Options* screen. If you have not created
a *EC2 keypair*, it is recommended that you do so now. This will allow you to
access the Yarn, Spark, and Ganglia web interfaces in your browser. This makes
debugging and monitoring the execution of your Spark Job much more manageable.
To create a *EC2 keypair*, follow [these instructions][keypair instructions].

After this has all been completed you are ready to spin up your first cluster
by pressing *Create cluster*. Once the cluster has been created, AWS will start
provisioning machines. This should take about 10 minutes. In the meantime you
can add a step. Go the *Steps* foldout, and select *Spark application* for
*Step Type*. Clicking on *Configure* will open a dialogue in which you can
select the application JAR location in your S3 bucket, as well as any number
of argument to the application, spark-submit, as well as your action on
failure.

**Make sure you do not try to process the entire dataset in your
initial run, but, similar to lab 1, start with a few files, to confirm that the
application works as intended**

The setup will take some time to finish, so in the meantime you should
configure a proxy for the web interfaces. More detailed information can be
found on the [AWS website]. You can check the logs in your S3 bucket, or the
web interfaces to track the progress of your application and whether any errors
have occurred.

By forwarding the web interfaces you will also have access to Apache Ganglia.
Ganglia is a tool that allows you to monitor your cluster for incoming and
outgoing network, CPU load, memory pressure, and other useful metrics. They can
help to characterize the workload at hand, and help optimizing computation
times. An example of its interface is shown in [@fig:ganglia].

![Ganglia screenshot](./images/ganglia.png){#fig:ganglia}

It's not uncommon to run into problems when you first deploy your application
to AWS, here are some general clues:

-   You can access S3 files directly using Spark, so via
    `SparkContext.textFile` and `SparkSession.read.csv`, but not using the OS,
    so using an ordinary `File` java class will not work. If you want to load a
    file to the environment, you will have to figure out a workaround.

-   You can monitor the (log) output of your master and worker nodes in Yarn,
    which you can access in the web interfaces. It might help you to insert
    some helpful logging messages in your Application.

-   Scale your application by increasing the workload by an order of magnitude
    at a time, some bugs only become apparent when you have a sufficient load
    on your cluster and a sufficient cluster size. In terms of cost, it's also
    much cheaper if you do your debugging incrementally on smaller clusters.

-   Ensure that your cluster is running in actual cluster mode (can be visually
    confirmed by checking the load on the non-master nodes in Ganglia).


## Assignment

For this lab, we would like you to process the entire dataset, meaning all
segments, with 20 `c4.8xlarge` core nodes, in under half an hour, using your
solution from lab 1. This should cost you less than 12 dollars and is the
minimum requirement.

Note that this means that you should evaluate whether you application scales
well enough to achieve this before attempting to run it on the entire dataset.
Your answers to the questions in lab 1 should help you to determine this, but
to reiterate: consider e.g.Â how long it will take to run

1. 1000 segments compared to 10000 segments?
2. on 4 virtual cores compared to 8, and what about 32?

If your application is not efficient enough right away, you should analyze its
bottlenecks and modify it accordingly, or try to gain some extra performance by
modifying the way Spark is configured.
You can try a couple of runs on the entire dataset when you have a good
understanding of what might happen on the entire dataset.

For extra points, we challenge you to come up with an even better solution
according to the metric you defined in lab 1. You are free to change anything,
but some suggestions are:

-   Find additional bottlenecks using Apache Ganglia (need more network I/O, or
    more CPU?, more memory?)
-   Tuning the kind and number of machines you use on AWS, based on these
    bottlenecks
-   Modifying the application to increase performance
-   Tuning Yarn/Spark configuration flags to best match the problem

There is a [guide to Spark performance] tuning on the Spark website.


## Deliverables

-   A report outlining your choices in terms of configuration and your results.
-   A presentation (maximum 5 slides/minutes) in which you present your work
    and results to the class. Try to put an emphasis on the
    improvements you found, what kind of settings/configurations/changes had
    the most impact.

In the report, there should be a justification for why you chose the cluster
configuration you did. If you have measurements for multiple cluster
configurations please include them. Also detail all the improvements you found,
and why they improved effectiveness.

  [EC2]: https://aws.amazon.com/ec2/
  [EMR]: https://aws.amazon.com/emr/
  [S3]: https://aws.amazon.com/s3/
  [US east region]: https://aws.amazon.com/public-datasets/common-crawl/
  [EC2 website]: https://aws.amazon.com/ec2/spot/pricing/
  [keypair instructions]:
  https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-key-pairs.html
  [Amazon web interfaces]:
  https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-web-interfaces.html
  [guide to Spark performance]: https://spark.apache.org/docs/latest/tuning.html
