# Lab 3 {.unnumbered}

In the third and final lab of SBD we will be implementing a streaming
application. As many of you have noted in the first lab questions, Spark is not
well suited for real-time streaming, because of its batch-processing nature.
Therefore, we will be using *Apache Kafka* for this lab. You will be provided
with a Kafka stream of GDELT records, for which we want you to create a
histogram of the most popular topics of the last hour that will continuously
update. We included another visualizer for this lab that you can see in
[@fig:stream_visualizer].

![Visualizer for the streaming
application](./images/stream_visualizer){#fig:stream_visualizer}

Apache Kafka is a distributed streaming platform. The core abstraction is that
of a message queue, to which you can both publish and subscribe to streams of
records. Each queue is named by means of a topic. Apache Kafka is:

-   Resilient by means of replication;
-   Scalable on a cluster;
-   High-throughput and low-latency; and
-   A persistent store.

Kafka consists of 4 APIs, from the Kafka docs:

The Producer API
:   allows an application to publish a stream of records to one or more Kafka
    topics.

The Consumer API
:   allows an application to subscribe to one or more topics and process the
    stream of records produced to them.

The Streams API
:   allows an application to act as a stream processor, consuming an input
    stream from one or more topics and producing an output stream to one or
    more output topics, effectively transforming the input streams to output
    streams.

The Connector API
:   allows building and running reusable producers or consumers that connect
    Kafka topics to existing applications or data systems. For example, a
    connector to a relational database might capture every change to a table.

Before you start with the lab, please read the [Introduction to Kafka on the Kafka
website](https://kafka.apache.org/intro), to become familiar with the Apache
Kafka abstraction and internals. You can find instructions on how to [install
Kafka on your machine here](https://kafka.apache.org/quickstart). A good
introduction to the [Kafka stream API can be found
here](https://docs.confluent.io/current/streams/quickstart.html). We recommend
you go through the code and examples.

We will again be using Scala for this assignment. Although Kafka's API is
completely written in Java, the streams API has been wrapped in a Scala API for
convenience. You can find the Scala KStreams documentation
[here](https://developer.lightbend.com/docs/api/kafka-streams-scala/0.2.1/com/lightbend/kafka/scala/streams/KStreamS.html),
for API docs on the different parts of Kafka, like `StateStores`, please refer
to [this link](https://kafka.apache.org/20/javadoc/overview-summary.html).

## Setting up

In the lab's repository you will find a template for your solution. There are a
bunch of scripts (`.sh` for MacOS/Linux, `.bat` for Windows). For these scripts
to work you first will have to define a `KAFKA_HOME` environment variable to
the root of the Kafka installation directory. The Kafka installation directory
should contain the following directories:

```
$ tree .
├── bin
│   └── windows
├── config
├── libs
└── site-docs
```

Once that has been set up, copy the lab files from the GitHub repository. Try
to run the `kafka_start.sh` or `kafka_start.bat` depending on your OS. If you
receive an error about being unable to find a `java` binary, make sure you have
Java installed and it is in your path.

The `kafka_start` script does a number of things:

1. Start a Zookeeper server, which acts as a naming, configuration and task
   coordination server, on port 2181
2. Start a single Kafka broker on port 9092

Navigate to the GDELTProducer directory, and run `sbt run` to
start[^start_twice] the GDELT stream.

[^start_twice]: As this [issue](https://github.com/sbt/sbt/issues/3618)
suggests, you might need to run `sbt run` twice when starting the producer for
the first time.


We can now inspect the output of the `gdelt` topic by running the following
command on MacOS/Linux:
```
$KAFKA_HOME/bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 \
    --topic gdelt --property print.key=true --property key.separator=-
```

Or on Windows PowerShell:

```
$env:KAFKA_HOME\bin\windows\kafka-console-consumer.bat --bootstrap-server localhost:9092 `
        --topic gdelt --property print.key=true --property key.separator=-
```

Or on Windows cmd:
```
%KAFKA_HOME%\bin\windows\kafka-console-consumer.bat --bootstrap-server localhost:9092 ^
        --topic gdelt --property print.key=true --property key.separator=-
```

If you see output appearing, you are now ready to start on the assignment.

## Assignment

As mentioned before, for this assignment, we will no longer batch process the
GDELT Global Knowledge Graph, but rather stream it into a pipeline that
computes a histogram of the last hour. This pipeline is depicted by
[@fig:kafka_pipeline]. We will give a small description of the individual parts
below.

![GDELT streaming pipeline](./images/kafka_pipeline){#fig:kafka_pipeline}

Producer
:   The producer, contained in the `GDELTProducer` Scala project, starts by
downloading all segments of the previous hour (minus a 15 minute offset), and
immediately start streaming records (rows) to a Kafka topic called `gdelt`.
Simultaneously, it will schedule a new download step at the next quarter of the
hour. The frequency by which the records are streamed is determined as the current
amount of queued records over the time left until new data is downloaded from
S3.

Transformer
:   The transformer receives GDELT records on the `gdelt` topic and should use
them to construct a histogram of the names from the "allNames" column of the
dataset, but only for the last hour. This is very similar to the application
you wrote in Lab 1, but it happens in real-time and you should take care to
also decrement/remove names that are older than an hour (relative to your input
data). Finally, the transformer's output should appear on a Kafka topic called
`gdelt-histogram`.

Consumer
:   The consumer finally acts as a _sink_, and will process the incoming
histogram updates from the transformer into a smaller histogram of only the 100
most occurring names for display [^downsample]. It will finally stream this
histogram to
our visualizer over a WebSocket connection.

[^downsample]: It might turn out that this is too much for your browser to
handle. If this is the case, you may change it manually in the
`HistogramProcessor` contained in `GDELTConsumer.scala`.

You are now tasked with writing an implementation of the histogram transformer.
In the file `GDELTStream/GDELTStream.scala` you will have to implement the
following

GDELT row processing
:   In the main function you will first have to write a function that filters
    the GDELT lines to a stream of allNames column. You can achieve this using
    the high-level API of Kafka Streams, on the `KStream` object.

HistogramTransformer
:   You will have to implement the `HistogramTransformer` using the
    processor/transformer API of kafka streams, to convert the stream of
    allNames into a histogram of the last hour. We suggest you look at [state
    store for Kafka streaming].

  [state store for Kafka streaming]: https://kafka.apache.org/20/documentation/streams/developer-guide/processor-api.html

You will have to write the result of this stream to a new topic called
`gdelt-histogram`.
This stream should consist of records (key-value pairs) of the form `(name,
count)` and type `(String, Long)`, where the value of name was extracted from
the "allNames" column.

This means that whenever the transformer reads a name from the "allNames"
column, it should publish an updated, i.e. incremented, (name, count) pair on
the output topic, so that the visualizer can update accordingly. When you
decrement a name, because its occurrence is older than an hour, remember to
publish an update as well!

To run the visualizer, first start the websocket server by navigating to the
GDELTConsumer directory and running `sbt run`. Next, navigate to the
visualization directory in the root of the GitHub repository, under assignment
3, open index.html. Once that is opened, press open web socket to start
the visualization.


## Deliverables

-   A complete zip of the entire project, including your implementation of
    `GDELTStream.scala` (please remove all data files from the zip!)
- A report containing
    - Outline of the code (less than 1/2 a page)
    - Answers to the questions listed below

## Questions

Try to be concise with your answers. Some questions have a maximum number of
words you can use, but you are welcome to use fewer if you can.

### General Kafka questions

1.  What is the difference, in terms of data processing, between Kafka and Spark?
2.  What is the difference between replications and partitions?
3.  What is Zookeeper's role in the Kafka cluster? Why do we need a separate
    entity for this? (max. 50 words)
4.  Why does Kafka by default not guarantee *exactly once* delivery semantics
    on producers? (max. 100 words)
5.  Kafka is a binary protocol (with a reference implementation in Java),
    whereas Spark is a framework. Name two (of the many) advantages of Kafka
    being a binary protocol in the context of Big Data. (max. 100 words)

### Questions specific to the assignment

1.  On average, how many bytes per second does the stream transformer have to
    consume? How many does it produce?
2.  Could you use a Java/Scala data structure instead of a Kafka State Store to
    manage your state in a processor/transformer? Why, or why not?
    (max. 50 words)
3.  Given that the histogram is stored in a Kafka `StateStore`, how would you
    extract the top 100 topics? Is this efficient? (max. 75 words)
4.  The visualizer draws the histogram in your web browser. A Kafka consumer
    has to communicate the current 'state' of the histogram to this visualizer.
    What do you think is an efficient way of streaming the 'state' of the
    histogram to the webserver?
    (max. 75 words)
5.  What are the two ways you can scale your Kafka implementation over multiple
    nodes? (max. 100 words)
6.  How could you use Kafka's partitioning to compute the histogram in
    parallel? (max. 100 words)
